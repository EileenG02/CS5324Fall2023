{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "# MacOS Environment\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Image Manipulation\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall, F1Score\n",
    "\n",
    "# Warnings\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 :0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4\n",
    "ROW_COUNT, COLUMN_COUNT = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "# Sequence input with equivale\n",
    "sequence_input = Input(shape=(COLUMN_COUNT,))\n",
    "\n",
    "# this will reduce the input dimension from VOCAB_SIZE to 50 for each word\n",
    "# the lenght will be the maximum number of words in a document, so 500\n",
    "embedded_sequences = Embedding(top_words, # input dimension (max int of OHE)\n",
    "                               EMBEDDING_SIZE, # output dimension size\n",
    "                               input_length=max_review_length)(sequence_input) # number of words in each sequence\n",
    "\n",
    "# starting sequence size is 500 (words) by 50 (embedded features)\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, length becomes: 500-4=496 \n",
    "# so overall size is 496 by 64\n",
    "\n",
    "# now pool across time\n",
    "x = MaxPooling1D(5)(x)# after max pool, 496/5 -> 99 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# extract additional features\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 95 after the conovlutions\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# more features through CNN processing!\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# after convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 64 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten \n",
    "\n",
    "# Take the variance of these elements across features, result is 64 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "x = Dense(64, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='conv_model_small_embedding.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True) \n",
    "# clipnorm=1.0, \n",
    "\n",
    "opt = Adam(epsilon=0.0001, learning_rate=lr_schedule)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy', Precision(), Recall(), F1Score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "history_cnn_low_embedding = []\n",
    "tmp = model.fit(X_train, y_train, epochs=5, \n",
    "                batch_size=64, \n",
    "                validation_data=(X_test, y_test))\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in history_cnn_low_embedding])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['F1Score'])\n",
    "plt.plot(combined['val_F1Score'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
