{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 7: Convolutional Network Architectures - Brain Tumor MRI Images**\n",
    "\n",
    "- Reece Iriye: 48255107\n",
    "- Eileen Garcia: 48241821\n",
    "- Trevor Dohm: 48376059\n",
    "\n",
    "## **0: Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Embedding, Dense, Dropout, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1: Data Preparation and Preprocessing**\n",
    "\n",
    "### **1.1: Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the training dataset ('training.1600000.processed.noemoticon.csv') using Pandas and display the first few rows. Displaying the first few rows helps us understand the data structure and content that we will work with. \n",
    "\n",
    "The output shows that each row in the collection of tweets includes a sentiment identifier, an id, a date, a flag, a user, and the tweet text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Datasets\n",
    "dataset = 'Dataset/training.1600000.processed.noemoticon.csv'\n",
    "data = pd.read_csv(dataset, header=None, names=[\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"], encoding = 'ISO-8859-1')\n",
    "\n",
    "# Display First Few Rows For Each Daatset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target \"sentiment\" identifier represents the polarity of the tweet, where 0 is negative, and 4 is positive. \n",
    "\n",
    "The \"Id\" represents the id of the tweet. \n",
    "\n",
    "The \"date\" is the date that the tweet was published. \n",
    "\n",
    "The \"flag\" represents the query, and if there is no query, the value is \"NO_QUERY\".\n",
    "\n",
    "The \"user\" column represents the username that published the tweet.\n",
    "\n",
    "The \"text\" is the text that was posted as a tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a tweet may include mentions to other users, special characters, and numbers, we need to remove some of these markers to have more standardized data. \n",
    "\n",
    "In the code below, we remove the mentions (formatted as usernames starting with @), URLs, special characters, and numbers, to leave only alphabetic characters. \n",
    "\n",
    "The text is also converted to lower case, and leading or trailing whitespaces are removed. This process further standardizes the text data and results in a new column \"clean_text\" that contains that cleaned versions of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sentiment\n",
       " 0    800000\n",
       " 4    800000\n",
       " Name: count, dtype: int64,\n",
       "                                           clean_text  sentiment\n",
       " 0  awww thats a bummer  you shoulda got david car...          0\n",
       " 1  is upset that he cant update his facebook by t...          0\n",
       " 2  i dived many times for the ball managed to sav...          0\n",
       " 3     my whole body feels itchy and like its on fire          0\n",
       " 4  no its not behaving at all im mad why am i her...          0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean Text Regex\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove Mentions, URL\n",
    "    text = re.sub(r'(@[A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)', ' ', text)\n",
    "\n",
    "    # Remove Special Characters, Numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Convert To Lower Case\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Return Cleaned Text\n",
    "    return text\n",
    "\n",
    "# Apply Cleaning Function To Text Column\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Explore Target Column\n",
    "target_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Display First Few Rows Of Cleaned Text, Target Distribution\n",
    "clean_text_head = data[['clean_text', 'sentiment']].head()\n",
    "target_counts, clean_text_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning, we explored the target column: sentiment. The values in the column are counted to help understand the distribution of sentiments in the dataset. The output shows that there are exactly 800,000 tweets categorized as negative (0), and the same amount as positive (4). The sentiment classes in this dataset are perfectly balanced. \n",
    "\n",
    "Finally we display the first few rows of the text along with their sentiments. We see a few tweets categorized as \"negative\". \n",
    "\n",
    "The data is almost ready for tokenization, vectorization, and feeding it into a neural network for sentiment classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2: Choosing an Evaluation Metric**\n",
    "\n",
    "In the context of Twitter sentiment analysis for understanding sentiment of a specific product using data on a variety of random topics, selecting appropriate evaluation metrics is crucial to ensure the model's reliability and practical usability. Given the nature of sentiment analysis, where the goal is to gauge public sentiment accurately, a combination of Accuracy, F1-Score, and Confusion Matrix offers a comprehensive evaluation approach.\n",
    "\n",
    "Accuracy, first off, is a straightforward measure of how often the model predicts correctly. In a dataset with a balanced class distribution, as in this case where there’s 800,000 positive Tweets and 800,000 negative Tweets, accuracy becomes a relevant metric because it gives a clear indication of the model's overall performance. A high accuracy rate in a balanced dataset means the model performs well across both positive and negative sentiments, which is essential for businesses to accurately assess public opinion.\n",
    "\n",
    "F1-Score is particularly important in sentiment analysis because it balances the precision and recall of the classifier. This balance is crucial in a business context where both identifying positive sentiments (precision) and not missing negative sentiments (recall) are equally important. A high F1-Score indicates that the model is not only capturing most of the relevant sentiment but also maintaining a low rate of false positives, which is vital for creating a reliable sentiment analysis tool.\n",
    "A Confusion Matrix provides detailed insight into the model's performance by showing the true positives, false positives, true negatives, and false negatives. This level of detail is valuable in this context, because it’s a more graphical description of the F1-Score for Twitter Sentiment analysis. It helps us see where exactly the positive sentiment predictions and negative sentiment predictions relate to the actual reality, and it lets us identify exactly where this is the case instead of just looking at the data. For instance, a high number of false negatives might indicate that the model is underestimating negative sentiment, which could be critical for a context like customer service if this model were to be applied in that realm. \n",
    "\n",
    "Having broad applicability would require that this model performs well with all of these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3: Choosing a Method for Splitting Our Data**\n",
    "\n",
    "\n",
    "The distribution is evenly distributed. There is almost a 50% split for positive and negative. Thus, we will do an 80-20 split. The reason for this is that we have a large amount of data evenly distributed across two classes, and because of this phenomenon, a class imbalance would be extremely unlikely to occur. \n",
    "\n",
    "With 1.6 million tweets evenly split between positive and negative sentiments, your dataset is substantial enough to allow for an 80-20 split without risking the loss of representativeness in either the training or testing sets. This large volume of data ensures that both subsets (training and testing) will likely maintain the same distribution of sentiments as the original dataset.\n",
    "\n",
    "An 80-20 split minimizes the risk of class imbalance in both training and testing sets. This balance is crucial in training the model to perform equally well on both classes of sentiment, which is essential for a business application where understanding both positive and negative consumer sentiments is vital.\n",
    "\n",
    "By allocating 80% of the data to training, we ensure that the model has enough examples to learn from, which is crucial for developing a strong and flexible sentiment analysis model. The remaining 20% for testing is also substantial enough to reliably evaluate the model's performance across a wide range of examples, ensuring that the model's accuracy and generalizability are well-tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we perform the train/test split as described above. \n",
    "\n",
    "First, we re-encode the labels. Originally, a 0 represents negative sentiment and 4 represents positive sentiment. We convert these sentiment labels into a binary format where 1 represents positive sentiments and 0 remains as the negative sentiment. The new encodings are stored in a new column called \"target_encoded\".\n",
    "\n",
    "We then set the \"clean_text\" column containing the processed text of tweets as the features. We set the new \"target_encoded\" column representing the binary sentiment labels as the labels. \n",
    "\n",
    "As described above, we perform a 80/20 train/test split with a set random_state to ensure reproducibility of the split. \n",
    "\n",
    "Finally, we check the shapes of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1280000,), (320000,), (1280000,), (320000,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode Labels: Convert 4 -> 1 For Positive Sentiment\n",
    "\n",
    "data['target_encoded'] = data['sentiment'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Split data into features (text) and labels\n",
    "\n",
    "# Split Data Into Features \n",
    "features = data['clean_text']\n",
    "labels = data['target_encoded']\n",
    "\n",
    "# Perform 80 / 20 Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Check Shapes\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape output tells us the following:\n",
    "\n",
    "- There are 1,280,000 samples in the training set for features (clean_text).\n",
    "- There are 320,000 samples in the testing set for features.\n",
    "- There are 1,280,000 labels corresponding to the training set.\n",
    "- There are 320,000 labels corresponding to the testing set.\n",
    "\n",
    "The data has been successfully split into training and testing sets with the intended proportion and that each feature has a corresponding label in both the training and testing sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4: Tokenizing and Padding the Dataset**\n",
    "\n",
    "Next, we need to tokenize and pad the training dataset. Tokenization converts the tweet texts to sequences of integers, and the we pad the sequences to a fixed length. \n",
    "\n",
    "We set a few constants first:\n",
    "- NUM_TOP_WORDS: Set to None, meaning that the tokenizer will consider all unique words in the dataset.\n",
    "- MAX_ART_LEN: Set to 40 to specify the maximum length of the sequences. Any text sequence longer than 40 will be truncated and shorter sequences will be padded.\n",
    "- NUM_CLASSES: Set to 2, to represent the two classes in the target variable (positive and negative sentiments).\n",
    "\n",
    "Then we intialize and fit the tokenizer. A tokenizer is a tool to convert text into a sequence of integers, so that each integer represents a specific word. This tokenizer will consider all words, since we set `NUM_TOP_WORDS` to 'None'. The tokenzier is then fit on `X_train`, allowing it to learn the mapping of words to integers based on the training data. \n",
    "\n",
    "We also store a `word_index` variable, which is a dictionary where keys are words and values are their corresponding integers in the learned vocabulary. \n",
    "\n",
    "Lastly, we convert the training and testing text data to sequences of integers based on the learned word index. Then we ensure that all sequences are the same length by padding shorter sequences with zeros and truncates longer sequences to this standardized length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Maximum Sequence Length\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = 40 \n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Initialize, Fit Tokenizer\n",
    "tokenizer = Tokenizer(num_words = NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Convert Text To Sequence, Padding\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded = pad_sequences(train_sequences, maxlen = MAX_ART_LEN)\n",
    "X_test_padded = pad_sequences(test_sequences, maxlen = MAX_ART_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing GloVe Embedding\n",
    "glove_file = 'Dataset/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "with open(glove_file, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s Word Vectors.' % len(embeddings_index))\n",
    "\n",
    "# Create Embedding Matrix\n",
    "found_words = 0\n",
    "EMBED_SIZE = 100\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, EMBED_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words + 1\n",
    "\n",
    "# Print Embedding Information\n",
    "print(\"Embedding Shape:\", embedding_matrix.shape,\n",
    "      \"\\nTotal Words Found:\", found_words,\n",
    "      \"\\nPercentage:\", 100 * found_words / embedding_matrix.shape[0])\n",
    "\n",
    "# Check Shapes Of Padded Train Test Data\n",
    "X_train_padded.shape, X_test_padded.shape, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Embedding\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = MAX_ART_LEN,\n",
    "                            trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
