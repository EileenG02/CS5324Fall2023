{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Embedding, Dense, Dropout, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   2401  Borderlands  Positive  \\\n",
       " 0  2401  Borderlands  Positive   \n",
       " 1  2401  Borderlands  Positive   \n",
       " 2  2401  Borderlands  Positive   \n",
       " 3  2401  Borderlands  Positive   \n",
       " 4  2401  Borderlands  Positive   \n",
       " \n",
       "   im getting on borderlands and i will murder you all ,  \n",
       " 0  I am coming to the borders and I will kill you...     \n",
       " 1  im getting on borderlands and i will kill you ...     \n",
       " 2  im coming on borderlands and i will murder you...     \n",
       " 3  im getting on borderlands 2 and i will murder ...     \n",
       " 4  im getting into borderlands and i can murder y...     ,\n",
       "    3364   Facebook Irrelevant  \\\n",
       " 0   352     Amazon    Neutral   \n",
       " 1  8312  Microsoft   Negative   \n",
       " 2  4371      CS-GO   Negative   \n",
       " 3  4433     Google    Neutral   \n",
       " 4  6273       FIFA   Negative   \n",
       " \n",
       "   I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£  \n",
       " 0  BBC News - Amazon boss Jeff Bezos rejects clai...                                                                                                                                                                                                  \n",
       " 1  @Microsoft Why do I pay for WORD when it funct...                                                                                                                                                                                                  \n",
       " 2  CSGO matchmaking is so full of closet hacking,...                                                                                                                                                                                                  \n",
       " 3  Now the President is slapping Americans in the...                                                                                                                                                                                                  \n",
       " 4  Hi @EAHelp Iâ€™ve had Madeleine McCann in my cel...                                                                                                                                                                                                  )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load Datasets\n",
    "training_data_path = 'Dataset/twitter_training.csv'\n",
    "validation_data_path = 'Dataset/twitter_validation.csv'\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "validation_data = pd.read_csv(validation_data_path)\n",
    "\n",
    "# Display First Few Rows For Each Daatset\n",
    "training_data_head = training_data.head()\n",
    "validation_data_head = validation_data.head()\n",
    "(training_data_head, validation_data_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object),\n",
       " array(['Neutral', 'Negative', 'Positive', 'Irrelevant'], dtype=object),\n",
       " count    74681.000000\n",
       " mean        19.050120\n",
       " std         14.492693\n",
       " min          0.000000\n",
       " 25%          8.000000\n",
       " 50%         15.000000\n",
       " 75%         27.000000\n",
       " max        198.000000\n",
       " Name: im getting on borderlands and i will murder you all ,, dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Unique Sentiment Labels\n",
    "training_sentiments_fixed = training_data.iloc[:, 2].unique()\n",
    "validation_sentiments_fixed = validation_data.iloc[:, 2].unique()\n",
    "\n",
    "# Adjusting the code to handle NaN or non-string values in tweets\n",
    "training_tweet_lengths_fixed = training_data.iloc[:, 3].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "\n",
    "# Recalculating the summary statistics of tweet lengths in the training dataset\n",
    "tweet_length_summary_fixed = training_tweet_lengths_fixed.describe()\n",
    "\n",
    "(training_sentiments_fixed, validation_sentiments_fixed, tweet_length_summary_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "top_words = 5000\n",
    "max_sequence_length = 50\n",
    "\n",
    "# Fourth Column Contains Tweet Text\n",
    "tweet_column_index = 3\n",
    "\n",
    "# Convert all entries to strings\n",
    "training_texts = training_data.iloc[:, tweet_column_index].astype(str)\n",
    "validation_texts = validation_data.iloc[:, tweet_column_index].astype(str)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=top_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_texts)\n",
    "\n",
    "# Convert the tweets to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(training_texts)\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_texts)\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "X_validation_padded = pad_sequences(validation_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder and transform the sentiment labels to integers\n",
    "# Using the third column (index 2) for sentiment labels\n",
    "y_train = label_encoder.fit_transform(training_data.iloc[:, 2])\n",
    "y_validation = label_encoder.transform(validation_data.iloc[:, 2])\n",
    "\n",
    "# Ensure y_train and y_validation are numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_validation = np.array(y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 17:40:19.188254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:19.224794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:19.224861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:19.228667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:19.228722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:19.228744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:20.928141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:20.928199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:20.928209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-09 17:40:20.928244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-09 17:40:20.928295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5409 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 50, 32)            161600    \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 50, 32)            10656     \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 84        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173000 (675.78 KB)\n",
      "Trainable params: 173000 (675.78 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 17:40:28.095779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe740024a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-09 17:40:28.095821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2023-12-09 17:40:28.106199: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-09 17:40:28.428387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-12-09 17:40:28.517599: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Assuming NUM_CLASSES is the number of sentiment classes\n",
    "NUM_CLASSES = 4  # Replace with the actual number of sentiment classes\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(max_sequence_length,))\n",
    "x = TokenAndPositionEmbedding(max_sequence_length, top_words, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='softmax')(x)  # Using softmax for multi-class classification\n",
    "\n",
    "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(model_xformer.summary())\n",
    "\n",
    "# Compiling the model\n",
    "model_xformer.compile(optimizer='adam', \n",
    "                      loss='sparse_categorical_crossentropy',  # or 'categorical_crossentropy' if one-hot encoded\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model_xformer.fit(\n",
    "    X_train_padded, y_train, batch_size=64, epochs=2, \n",
    "    validation_data=(X_validation_padded, y_validation)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
