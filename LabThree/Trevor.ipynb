{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Combined Class Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "# Standard Stuff\n",
    "import numpy as np\n",
    "from numpy import ma # (Masked Array): Has NumPy Functions That Work With NaN Data\n",
    "from numpy.linalg import pinv # Moore-Penrose Pseudoinverse\n",
    "import math\n",
    "\n",
    "# Sklearn Imports \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import fmin_bfgs, minimize_scalar # BFGS Algorithm: Extremely Common Optimization Algorithm\n",
    "\n",
    "# Scipy Imports\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "class MultiClassLogisticRegression:\n",
    "\n",
    "    def __init__(self, eta, iterations = 20, C = 0.0001, solver = \"quasi\", penalty = \"l2\", line_iters = 10):\n",
    "        \"\"\"\n",
    "        MultiClassLogisticRegression Constructor\n",
    "\n",
    "        Note: Internally We Will Store The Weights As self.w_ To Keep With Sklearn Conventions\n",
    "        \n",
    "        Parameters:\n",
    "        - eta: Learning Rate\n",
    "        - iterations: Number Of Iterations (Default: 20)\n",
    "        - C: Regularization Strength (Default: 0.0001)\n",
    "        - solver: Optimization Algorithm: ('steepest', 'stochastic', 'newton', 'quasi') (Default: 'quasi')\n",
    "        - penalty: Regularization Penalty Type: ('none', 'l1', 'l2', 'both') (Default: 'l2')\n",
    "        - line_iters: Number Iterations For Line Search (If Applicable)\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.penalty = penalty\n",
    "        self.line_iters = line_iters\n",
    "        self.classifiers_ = []\n",
    "\n",
    "    # Return Trained Status (String)\n",
    "    def __str__(self):\n",
    "        if hasattr(self, 'w_'):\n",
    "            return 'MultiClass Logistic Regression Object With Coefficients:\\n' + str(self.w_)\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "\n",
    "    # ==============================\n",
    "    # Convenience Methods (Private):\n",
    "    # ==============================\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "\n",
    "        # Add Bias Term\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "\n",
    "        # Increase Stability, Redefine Sigmoid Operation (1 / (1 + np.exp(-theta)))\n",
    "        return expit(theta)\n",
    "    \n",
    "    # Defines Function With First Input To Be Optimized (Eta)\n",
    "    @staticmethod\n",
    "    def _objective_function_steepest(eta, X, y, w, grad, C):\n",
    "        wnew = w - grad * eta\n",
    "        g = expit(X @ wnew)\n",
    "\n",
    "        # The Line Search Looks For Minimization, Take Negative Of Log Likelihood\n",
    "        return -np.sum(ma.log(g[y == 1])) - ma.sum(np.log(1 - g[y == 0])) + C * sum(wnew**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _objective_function_bfgs(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "\n",
    "        # Invert This Because SciPy Minimizes, But We Derived All Formulas For Maximizing\n",
    "        return -np.sum(ma.log(g[y == 1])) - np.sum(ma.log(1 - g[y == 0])) + C * sum(w**2) \n",
    "\n",
    "    @staticmethod\n",
    "    def _objective_gradient_bfgs(w, X, y, C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y - g\n",
    "        gradient = np.mean(X * ydiff[:, np.newaxis], axis = 0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # ============================\n",
    "    # Binary Probability Functions\n",
    "    # ============================\n",
    "\n",
    "    def _predict_proba_binary(self, X, w, add_bias = True):\n",
    "\n",
    "        # Add Bias Term If Requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "\n",
    "        # Return Probability Of Class 1\n",
    "        return self._sigmoid(Xb @ w)\n",
    "    \n",
    "    def _predict_binary(self, X):\n",
    "\n",
    "        # Return Actual Prediction\n",
    "        return (self._predict_proba_binary(X, self.w_) > 0.5)\n",
    "    \n",
    "    # ==================\n",
    "    # Gradient Functions\n",
    "    # ==================\n",
    "\n",
    "    # Vectorized Gradient Calculation With Chosen Regularization\n",
    "    def _get_gradient(self, X, y):\n",
    "\n",
    "        # Get Difference, Make Column Vector And Multiply Through\n",
    "        ydiff = y - self._predict_proba_binary(X, self.w_, add_bias = False).ravel()\n",
    "        gradient = np.mean(X * ydiff[:, np.newaxis], axis = 0)\n",
    "        \n",
    "        # Regularization\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.penalty == \"l1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.penalty == \"l2\":\n",
    "            gradient[1:] += -2 * self.C * self.w_[1:]\n",
    "        elif self.penalty == \"both\":\n",
    "            gradient[1:] += -self.C * (np.sign(self.w_[1:]) + 2 * self.w_[1:])\n",
    "        \n",
    "        # Return Gradient\n",
    "        return gradient\n",
    "    \n",
    "    # Stochastic Gradient Calculation\n",
    "    def _get_gradient_stochastic(self, X, y):\n",
    "\n",
    "        # Grab Random Instance, Get Difference, Make Column Vector And Multiply Through\n",
    "        idx = int(np.random.rand() * len(y))\n",
    "        ydiff = y[idx] - self._predict_proba_binary(X[idx], self.w_, add_bias = False) \n",
    "        gradient = X[idx] * ydiff[:, np.newaxis]\n",
    "        \n",
    "        # Regularization\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.penalty == \"l1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.penalty == \"l2\":\n",
    "            gradient[1:] += -2 * self.C * self.w_[1:]\n",
    "        elif self.penalty == \"both\":\n",
    "            gradient[1:] += -self.C * (np.sign(self.w_[1:]) + 2 * self.w_[1:])\n",
    "        \n",
    "        # Return Gradient\n",
    "        return gradient\n",
    "    \n",
    "    # Newton's Method Gradient Calculation\n",
    "    def _get_gradient_newton(self, X, y):\n",
    "\n",
    "        # Get Sigmoid Value For All Classes, Calculate Hessian\n",
    "        # Get Difference, Make Column Vector And Multiply Through\n",
    "        g = self._predict_proba_binary(X, self.w_, add_bias = False).ravel()\n",
    "        hessian = X.T @ np.diag(g * (1 - g)) @ X - 2 * self.C\n",
    "        ydiff = y - g\n",
    "        gradient = np.sum(X * ydiff[:, np.newaxis], axis = 0)\n",
    "        \n",
    "        # Regularization\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.penalty == \"l1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.penalty == \"l2\":\n",
    "            gradient[1:] += -2 * self.C * self.w_[1:]\n",
    "        elif self.penalty == \"both\":\n",
    "            gradient[1:] += -self.C * (np.sign(self.w_[1:]) + 2 * self.w_[1:])\n",
    "        \n",
    "        # Return Gradient\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    # ==============\n",
    "    # Fit Functions\n",
    "    # ==============\n",
    "\n",
    "    def _fit_binary(self, X, y):\n",
    "\n",
    "        # Add Bias Term\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        # Initialize Weight Vectors To Zeros\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        \n",
    "        # For Each Iteration (As Many As Max Iterations)\n",
    "        for _ in range(self.iters):\n",
    "            \n",
    "            # Depending On Solver, Change Gradient Calculation\n",
    "            if self.solver == \"stochastic\":\n",
    "                # print(\"Stochastic Gradient Ascent Method\")\n",
    "                gradient = self._get_gradient_stochastic(Xb, y)\n",
    "            elif self.solver == \"newton\":\n",
    "                # print(\"Newton Method\")\n",
    "                gradient = self._get_gradient_newton(Xb, y)\n",
    "            else:\n",
    "                gradient = self._get_gradient(Xb, y)\n",
    "\n",
    "            # Multiply By Learning Rate, Add B/C Maximizing\n",
    "            self.w_ += gradient*self.eta\n",
    "\n",
    "    def _fit_steepest(self, X, y):\n",
    "\n",
    "         # Add Bias Term\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        # Initialize Weight Vectors To Zeros\n",
    "        self.w_ = np.zeros((num_features, 1))\n",
    "        \n",
    "        # For Each Iteration (As Many As Max Iterations)\n",
    "        for _ in range(self.iters):\n",
    "\n",
    "            # Get Gradient (Use Base Solver Since Steepest)\n",
    "            # Minimization In Opposite Direction\n",
    "            gradient = -self._get_gradient(Xb, y)\n",
    "            \n",
    "            # Do Line Search In Gradient Direction, Using Scipy Function (FIXME Line Iters Choice ? What Would Be Optimal)\n",
    "            opts = {'maxiter' : self.line_iters}\n",
    "\n",
    "            # Minimize Scalar Function (Line Search)\n",
    "            res = minimize_scalar(self._objective_function_steepest,            # Objective Function To Optimize\n",
    "                                  bounds = (0, self.eta * 10),                  # Bounds To Optimize Over\n",
    "                                  args = (Xb, y, self.w_,gradient, self.C),     # Additional Argument For Objective Function\n",
    "                                  method = 'bounded',                           # Bounded Optimization For Speed\n",
    "                                  options = opts)                               # Set Maximum Iterations\n",
    "            \n",
    "            # Get Optimal Learning Rate, Set New Function Values, Subtract To Minimize\n",
    "            eta = res.x\n",
    "            self.w_ -= gradient * eta\n",
    "\n",
    "    def _fit_bfgs(self, X, y):\n",
    "\n",
    "        # Add Bias Term\n",
    "        Xb = self._add_bias(X)\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self._objective_function_bfgs,                      # What To Optimize\n",
    "                            np.zeros((num_features, 1)),                        # Starting Point\n",
    "                            fprime = self._objective_gradient_bfgs,             # Gradient Function\n",
    "                            args = (Xb, y, self.C),                             # Extra Arguments For Gradient, Objective Function\n",
    "                            gtol = 1e-03,                                       # Stopping Criteria For Gradient\n",
    "                            maxiter = self.iters,                               # Stopping Criteria Iterations\n",
    "                            disp = False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features, 1))\n",
    "    \n",
    "    # =================\n",
    "    # Public Functions:\n",
    "    # =================\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Initialize Probabilities\n",
    "        probs = []\n",
    "\n",
    "        # Get Probability For Each Classifier\n",
    "        for w in self.classifiers_:\n",
    "            probs.append(self._predict_proba_binary(X, w).reshape((len(X), 1)))\n",
    "\n",
    "        # Make Into Single Matrix\n",
    "        return np.hstack(probs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        # Take Argmax Along Row\n",
    "        return np.argmax(self.predict_proba(X), axis = 1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Store Unique Classes\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y))\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "\n",
    "        # For Each Unique Value\n",
    "        for i, yval in enumerate(self.unique_):\n",
    "\n",
    "            # Create Binary Problem\n",
    "            y_binary = np.array(y == yval).astype(int)\n",
    "            \n",
    "            # Train Binary Classifier For Class\n",
    "            if self.solver == \"steepest\":\n",
    "                # print(\"Steepest Ascent Method\")\n",
    "                self._fit_steepest(X, y_binary)\n",
    "            elif self.solver == \"quasi\":\n",
    "                # print(\"Quasi-Newton Method\")\n",
    "                self._fit_bfgs(X, y_binary)\n",
    "            else:\n",
    "                self._fit_binary(X, y_binary)\n",
    "\n",
    "            # Add Trained Classifier To List\n",
    "            self.classifiers_.append(self.w_)\n",
    "            \n",
    "        # Save All Weights Into One Matrix, Separate Column For Each Class\n",
    "        self.w_ = np.hstack([w for w in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultiClassLogisticRegression in module __main__:\n",
      "\n",
      "class MultiClassLogisticRegression(builtins.object)\n",
      " |  MultiClassLogisticRegression(eta, iterations=20, C=0.0001, solver='quasi', penalty='l2', line_iters=10)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, eta, iterations=20, C=0.0001, solver='quasi', penalty='l2', line_iters=10)\n",
      " |      MultiClassLogisticRegression Constructor\n",
      " |      \n",
      " |      Note: Internally We Will Store The Weights As self.w_ To Keep With Sklearn Conventions\n",
      " |      \n",
      " |      Parameters:\n",
      " |      - eta: Learning Rate\n",
      " |      - iterations: Number Of Iterations (Default: 20)\n",
      " |      - C: Regularization Strength (Default: 0.0001)\n",
      " |      - solver: Optimization Algorithm: ('steepest', 'stochastic', 'newton', 'quasi') (Default: 'quasi')\n",
      " |      - penalty: Regularization Penalty Type: ('none', 'l1', 'l2', 'both') (Default: 'l2')\n",
      " |      - line_iters: Number Iterations For Line Search (If Applicable)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split Into Train Test Sets (80 / 20 Split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Check Class Help\n",
    "help(MultiClassLogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object With Coefficients:\n",
      "[[-2.09332879 -0.93884016  1.89860074 -1.95702671 -1.79414871]\n",
      " [-1.00395114  0.34432948 -1.46008407  0.6501452  -1.13383002]\n",
      " [-2.99486496 -0.1293797  -0.16546227  2.08959157  3.3191224 ]]\n",
      "Accuracy: 0.90\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Instantiate Object\n",
    "lr = MultiClassLogisticRegression(eta = 0.1, iterations = 100, solver = 'steepest', penalty = 'l2')\n",
    "\n",
    "# Fit Model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print Coefficients\n",
    "print(lr)\n",
    "\n",
    "# Predict\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Other Tests (Not Trevor Code)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.29042546  0.66260688  0.74538491 -1.13375801 -0.75909516]\n",
      " [-0.29042546 -0.66260688 -0.74538491  1.13375801  0.75909516]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1.0,\n",
    "                                  iterations=4,\n",
    "                                  C=0.01,\n",
    "                                  solver=BFGSBinaryLogisticRegression,\n",
    "                                  penalty='l2'\n",
    "                                 )\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 15.79560094   1.9123695    1.14728782  -3.81520138  -7.45947451]\n",
      " [-15.79560094  -1.9123695   -1.14728782   3.81520138   7.45947451]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1.0,\n",
    "                                  iterations=5,\n",
    "                                  C=0.001,\n",
    "                                  solver=HessianBinaryLogisticRegression,\n",
    "                                  penalty='l1'\n",
    "                                 )\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='lbfgs',n_jobs=1,\n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',\n",
    "                          max_iter=50) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "\n",
    "lr_sk.fit(X_train, y_train) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# actually, we aren't quite as good as the lib linear implementation\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1, \n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',max_iter=100) \n",
    "\n",
    "lr_sk.fit(X_train,y_train) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(int) # make problem binary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.19967076]\n",
      " [-0.47455757]\n",
      " [-0.49940795]\n",
      " [ 0.77090226]\n",
      " [ 0.53602837]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.06296708]\n",
      " [-2.49138229]\n",
      " [-3.0503734 ]\n",
      " [ 4.23395947]\n",
      " [ 2.68939073]]\n",
      "Accuracy of:  1.0\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.25950273]\n",
      " [-0.59364614]\n",
      " [-0.61723524]\n",
      " [ 1.04556247]\n",
      " [ 0.71349414]]\n",
      "Accuracy of:  0.9333333333333333\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-12.02978114]\n",
      " [ -1.18083715]\n",
      " [ -0.13854738]\n",
      " [  1.98140576]\n",
      " [  6.15424339]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.19732186]\n",
      " [-0.47166791]\n",
      " [-0.58692298]\n",
      " [ 0.86137527]\n",
      " [ 0.5429108 ]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 35.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trevo\\AppData\\Local\\Temp\\ipykernel_48532\\906337921.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
      "c:\\Users\\trevo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2233: RuntimeWarning: invalid value encountered in subtract\n",
      "  r = (xf - nfc) * (fx - ffulc)\n",
      "c:\\Users\\trevo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2234: RuntimeWarning: invalid value encountered in subtract\n",
      "  q = (xf - fulc) * (fx - fnfc)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#\n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1, iterations=50,C=0.001)\n",
    "\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=1.0,\n",
    "                                    iterations=6, \n",
    "                                    line_iters=8, \n",
    "                                    C=0.001)\n",
    "\n",
    "lslr.fit(X_train,y_train)\n",
    "\n",
    "yhat = lslr.predict(X_test)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "slr = StochasticLogisticRegression(eta=0.01, iterations=800, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X_train,y_train)\n",
    "\n",
    "yhat = slr.predict(X_test)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "hlr = HessianBinaryLogisticRegression(eta=1.0,\n",
    "                                      iterations=4,\n",
    "                                      C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X_train,y_train)\n",
    "yhat = hlr.predict(X_test)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "bfgslr = BFGSBinaryLogisticRegression(_,iterations=3,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X_train,y_train)\n",
    "yhat = bfgslr.predict(X_test)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
