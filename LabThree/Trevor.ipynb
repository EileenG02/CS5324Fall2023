{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "# Standard Stuff\n",
    "import numpy as np\n",
    "from numpy import ma # (Masked Array): Has NumPy Functions That Work With NaN Data\n",
    "from numpy.linalg import pinv # Moore-Penrose Pseudoinverse\n",
    "\n",
    "# Sklearn Imports \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import fmin_bfgs # BFGS Algorithm: Extremely Common Optimization Algorithm\n",
    "\n",
    "# Scipy Imports\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Binary Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Binary Logistic Regression\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, penalty=\"l2\"):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.penalty = penalty # Regularization penalty type: \"none\", \"l1\", \"l2\", or \"both\"\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        # gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "\n",
    "        # add regularization (none does nothing)\n",
    "        if self.penalty == \"l1\":\n",
    "            gradient[1:] += -self.C * np.sign(self.w_[1:])\n",
    "        elif self.penalty == \"l2\":\n",
    "            gradient[1:] += -2 * self.C * self.w_[1:]\n",
    "        elif self.penalty == \"both\":\n",
    "            gradient[1:] += -self.C * (np.sign(self.w_[1:]) + 2 * self.w_[1:])\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Line Search Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Binary Logistic Regression To Use Line Search\n",
    "class LineSearchLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    # define custom line search for problem\n",
    "    def __init__(self, line_iters=0.0, **kwds):        \n",
    "        self.line_iters = line_iters\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "    \n",
    "    # this defines the function with the first input to be optimized\n",
    "    # therefore eta will be optimized, with all inputs constant\n",
    "    @staticmethod\n",
    "    def objective_function(eta,X,y,w,grad,C):\n",
    "        wnew = w - grad*eta\n",
    "        g = expit(X @ wnew)\n",
    "        # the line search is looking for minimization, so take the negative of l(w)\n",
    "        return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = -self._get_gradient(Xb,y)\n",
    "            # minimization is in opposite direction\n",
    "            \n",
    "            # do line search in gradient direction, using scipy function\n",
    "            opts = {'maxiter':self.line_iters} # unclear exactly what this should be\n",
    "            res = minimize_scalar(self.objective_function, # objective function to optimize\n",
    "                                  bounds=(0,self.eta*10), #bounds to optimize\n",
    "                                  args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                  method='bounded', # bounded optimization for speed\n",
    "                                  options=opts) # set max iterations\n",
    "            \n",
    "            eta = res.x # get optimal learning rate\n",
    "            self.w_ -= gradient*eta # set new function values\n",
    "            # subtract to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stochastic Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "    # stochastic gradient calculation \n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hessian Binary Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessianBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # just overwrite gradient function\n",
    "    def _get_gradient(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BFGS Binary Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFGS (Broyden–Fletcher–Goldfarb–Shanno) Algorithm Implemented \n",
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        gradient[1:] += -2 * w[1:] * C\n",
    "        return -gradient\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi Class Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow for the user to specify the algorithm they want to solver the binary case\n",
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=BFGSBinaryLogisticRegression,\n",
    "                 penalty = 'l2'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.penalty = penalty\n",
    "        self.classifiers_ = []\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 0.29042546  0.66260688  0.74538491 -1.13375801 -0.75909516]\n",
      " [-0.29042546 -0.66260688 -0.74538491  1.13375801  0.75909516]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1.0,\n",
    "                                  iterations=4,\n",
    "                                  C=0.01,\n",
    "                                  solver=BFGSBinaryLogisticRegression,\n",
    "                                  penalty='l2'\n",
    "                                 )\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[ 15.79560094   1.9123695    1.14728782  -3.81520138  -7.45947451]\n",
      " [-15.79560094  -1.9123695   -1.14728782   3.81520138   7.45947451]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1.0,\n",
    "                                  iterations=5,\n",
    "                                  C=0.001,\n",
    "                                  solver=HessianBinaryLogisticRegression,\n",
    "                                  penalty='l1'\n",
    "                                 )\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='lbfgs',n_jobs=1,\n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',\n",
    "                          max_iter=50) # all params default\n",
    "# note that sklearn is optimized for using the liblinear library with logistic regression\n",
    "# ...and its faster than our implementation here\n",
    "\n",
    "lr_sk.fit(X_train, y_train) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# actually, we aren't quite as good as the lib linear implementation\n",
    "# how do we compare now to sklearn?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sk = LogisticRegression(solver='liblinear',n_jobs=1, \n",
    "                           multi_class='ovr', C = 1/0.001, \n",
    "                           penalty='l2',max_iter=100) \n",
    "\n",
    "lr_sk.fit(X_train,y_train) # no need to add bias term, sklearn does it internally!!\n",
    "print(lr_sk.coef_)\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = (ds.target>1).astype(int) # make problem binary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.19967076]\n",
      " [-0.47455757]\n",
      " [-0.49940795]\n",
      " [ 0.77090226]\n",
      " [ 0.53602837]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-1.06296708]\n",
      " [-2.49138229]\n",
      " [-3.0503734 ]\n",
      " [ 4.23395947]\n",
      " [ 2.68939073]]\n",
      "Accuracy of:  1.0\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.25950273]\n",
      " [-0.59364614]\n",
      " [-0.61723524]\n",
      " [ 1.04556247]\n",
      " [ 0.71349414]]\n",
      "Accuracy of:  0.9333333333333333\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-12.02978114]\n",
      " [ -1.18083715]\n",
      " [ -0.13854738]\n",
      " [  1.98140576]\n",
      " [  6.15424339]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-0.19732186]\n",
      " [-0.47166791]\n",
      " [-0.58692298]\n",
      " [ 0.86137527]\n",
      " [ 0.5429108 ]]\n",
      "Accuracy of:  0.9666666666666667\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 35.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trevo\\AppData\\Local\\Temp\\ipykernel_48532\\906337921.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(ma.log(g[y==1]))-ma.sum(np.log(1-g[y==0])) + C*sum(wnew**2)\n",
      "c:\\Users\\trevo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2233: RuntimeWarning: invalid value encountered in subtract\n",
      "  r = (xf - nfc) * (fx - ffulc)\n",
      "c:\\Users\\trevo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2234: RuntimeWarning: invalid value encountered in subtract\n",
      "  q = (xf - fulc) * (fx - fnfc)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#\n",
    "\n",
    "blr = BinaryLogisticRegression(eta=0.1, iterations=50,C=0.001)\n",
    "\n",
    "blr.fit(X_train,y_train)\n",
    "print(blr)\n",
    "\n",
    "yhat = blr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "lslr = LineSearchLogisticRegression(eta=1.0,\n",
    "                                    iterations=6, \n",
    "                                    line_iters=8, \n",
    "                                    C=0.001)\n",
    "\n",
    "lslr.fit(X_train,y_train)\n",
    "\n",
    "yhat = lslr.predict(X_test)\n",
    "print(lslr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "slr = StochasticLogisticRegression(eta=0.01, iterations=800, C=0.001) # take a lot more steps!!\n",
    "\n",
    "slr.fit(X_train,y_train)\n",
    "\n",
    "yhat = slr.predict(X_test)\n",
    "print(slr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "hlr = HessianBinaryLogisticRegression(eta=1.0,\n",
    "                                      iterations=4,\n",
    "                                      C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "hlr.fit(X_train,y_train)\n",
    "yhat = hlr.predict(X_test)\n",
    "print(hlr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "\n",
    "#\n",
    "\n",
    "bfgslr = BFGSBinaryLogisticRegression(_,iterations=3,C=0.001) # note that we need only a few iterations here\n",
    "\n",
    "bfgslr.fit(X_train,y_train)\n",
    "yhat = bfgslr.predict(X_test)\n",
    "print(bfgslr)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
